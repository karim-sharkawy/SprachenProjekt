{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To enhance efficiency, you can make some optimizations and improvements to your code. Here are some suggestions:\n",
        "\n",
        "1. **Batch Requests**: Instead of sending individual HTTP requests for each language page, you can utilize batch processing or asynchronous requests to scrape multiple pages simultaneously, which can significantly reduce the overall execution time.\n",
        "\n",
        "2. **Caching Mechanism**: Implement a caching mechanism to store previously scraped data, reducing the need to re-scrape pages that haven't changed. This can be especially useful if you anticipate running the script multiple times.\n",
        "\n",
        "3. **Optimized XPath Queries**: Review the XPath queries used in your code and ensure they are as efficient as possible. Avoid overly broad queries that may unnecessarily traverse large portions of the HTML tree.\n",
        "\n",
        "4. **Minimize External Requests**: Minimize the number of external requests by fetching only the necessary data from each page. For example, if you only need language status, extract only that information instead of parsing the entire page.\n",
        "\n",
        "5. **Error Handling**: Implement robust error handling to gracefully handle exceptions, such as network errors or unexpected HTML structures. This ensures your script continues to run smoothly even in adverse conditions.\n",
        "\n",
        "6. **Parallel Processing**: Utilize parallel processing techniques, such as multi-threading or multiprocessing, to perform tasks concurrently and make better use of available system resources.\n",
        "\n",
        "7. **Code Refactoring**: Refactor your code to eliminate redundancy and improve readability. Look for opportunities to modularize repetitive tasks into functions or classes.\n",
        "\n",
        "Applying these optimizations should help improve the efficiency and performance of your code. If you'd like more specific guidance on implementing any of these suggestions, feel free to ask!"
      ],
      "metadata": {
        "id": "V0P9-ClLCAJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from lxml import html\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# gets the name of every language and their links\n",
        "def parse_languages(url):\n",
        "    response = requests.get(url)\n",
        "    tree = html.fromstring(response.content)\n",
        "\n",
        "    # XPath to find all <p> tags with class containing `lang--` and extract the text within the nested <a> tags\n",
        "    languages = tree.xpath('//p[contains(@class, \"lang--\")]/a/text()')\n",
        "    languages = [lang.strip() for lang in languages if lang.strip()]\n",
        "\n",
        "    # XPath to find the link for each language\n",
        "    language_links = tree.xpath('//p[contains(@class, \"lang--\")]/a/@href')\n",
        "    base_url = \"https://www.ethnologue.com\"  # Base URL of the website\n",
        "    full_language_links = [urljoin(base_url, link) for link in language_links]\n",
        "\n",
        "    return languages, full_language_links\n",
        "\n",
        "# ensures every page is parsed so no language is missed\n",
        "def find_links_to_other_pages(url):\n",
        "    response = requests.get(url)\n",
        "    tree = html.fromstring(response.content)\n",
        "\n",
        "    # XPath to find all <button> elements for different alphabet letters\n",
        "    links = tree.xpath('//button[@class=\"tab__link\"]/@onclick')\n",
        "    # Extract the URL part from the onclick attribute\n",
        "    links = [link.split('\"')[1] for link in links if 'browse' in link]\n",
        "    # Convert to full URLs\n",
        "    full_links = [urljoin(url, f'/browse/names/{link}') for link in links]\n",
        "\n",
        "    return full_links\n",
        "\n",
        "def main(start_url):\n",
        "    visited = set()\n",
        "    to_visit = [start_url]\n",
        "    all_languages = []\n",
        "    all_full_language_links = []  # Store all full language links\n",
        "\n",
        "    while to_visit:\n",
        "        current_url = to_visit.pop()\n",
        "        if current_url in visited:\n",
        "            continue\n",
        "\n",
        "        visited.add(current_url)\n",
        "        languages, full_language_links = parse_languages(current_url)\n",
        "        all_languages.extend(languages)\n",
        "        all_full_language_links.extend(full_language_links)  # Store full language links\n",
        "\n",
        "        links = find_links_to_other_pages(current_url)\n",
        "        to_visit.extend(links)\n",
        "\n",
        "    return all_languages, all_full_language_links\n",
        "\n",
        "start_url = 'https://www.ethnologue.com/browse/names/'\n",
        "languages, full_language_links = main(start_url)\n",
        "\n",
        "language_names = languages\n",
        "language_urls = full_language_links"
      ],
      "metadata": {
        "id": "iq1ihgjNXM_v",
        "outputId": "f2bd3c5b-2578-43ba-c092-425919cc1302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dd7dfcf39155>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mstart_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.ethnologue.com/browse/names/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mlanguages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_language_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mlanguage_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-dd7dfcf39155>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(start_url)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mall_full_language_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_language_links\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Store full language links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_links_to_other_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mto_visit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-dd7dfcf39155>\u001b[0m in \u001b[0;36mfind_links_to_other_pages\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# ensures every page is parsed so no language is missed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_links_to_other_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;31m# Redirect resolving generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;31m# Redirect resolving generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 resp = self.send(\n\u001b[0m\u001b[1;32m    267\u001b[0m                     \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m             )\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n\u001b[0m\u001b[1;32m    643\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0mcert_reqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcert_reqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mserver_hostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m     ssl_sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mca_certs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mca_cert_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mca_cert_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_verify_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mca_certs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_cert_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Function to scrape information from individual language pages\n",
        "def scrape_language_info(language_url):\n",
        "    try:\n",
        "        # Send a GET request to the language page\n",
        "        response = requests.get(language_url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        data = {}\n",
        "\n",
        "        # Extract language name\n",
        "        title_home = soup.find('h1', class_='title__home')\n",
        "        if title_home:\n",
        "            data['name'] = title_home.text.strip()\n",
        "\n",
        "        # Extract ISO 639 code\n",
        "        iso_code_element = soup.find('a', class_='chip chip--big')\n",
        "        if iso_code_element:\n",
        "            data['iso_code'] = iso_code_element.text.strip()\n",
        "\n",
        "        # Extract summary\n",
        "        summary_element = soup.find('section', id='summary')\n",
        "        if summary_element:\n",
        "            summary_p = summary_element.find('p')\n",
        "            if summary_p:\n",
        "                data['summary'] = summary_p.text.strip()\n",
        "\n",
        "        # Extract population info\n",
        "        population_element = soup.find('li', class_='population__sizes')\n",
        "        if population_element:\n",
        "            population_div = population_element.find('div', class_='graph__langpop')\n",
        "            if population_div:\n",
        "                data['population'] = population_div.text.strip()\n",
        "\n",
        "        # Extract language status (vitality)\n",
        "        vitality_element = soup.find('li', class_='population__vitality')\n",
        "        data['language_status'] = 'Unknown'  # Default value\n",
        "        if vitality_element:\n",
        "            vitality_status = vitality_element.find_all('li', class_='histogram__datum')\n",
        "            for status in vitality_status:\n",
        "                if 'data-count' in status.attrs and status['data-count'].isdigit() and int(status['data-count']) > 0:\n",
        "                    label = status.find('label')\n",
        "                    if label:\n",
        "                        data['language_status'] = label.text.strip()\n",
        "                        break\n",
        "\n",
        "        return data\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Request failed for URL {language_url}: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Introduce a delay of 3 seconds between each request\n",
        "delay_between_requests = 3\n",
        "\n",
        "# Combine language names and URLs into pairs\n",
        "language_info_pairs = zip(language_names, language_urls)\n",
        "\n",
        "# Dictionary to store language information\n",
        "language_info_dict = {}\n",
        "\n",
        "# Iterate over each language URL and scrape information\n",
        "for language_name, language_url in language_info_pairs:\n",
        "    language_info = scrape_language_info(language_url)\n",
        "    language_info_dict[language_name] = language_info\n",
        "    time.sleep(delay_between_requests)\n",
        "\n",
        "### turning into JSON file for javascript to read\n",
        "\n",
        "import json\n",
        "\n",
        "# Convert the dictionary to JSON\n",
        "json_data = json.dumps(language_info_dict)\n",
        "\n",
        "# Specify the file name\n",
        "file_name = \"language_info.json\"\n",
        "\n",
        "# Write JSON data to a file\n",
        "with open(file_name, \"w\") as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "# Print the name of the created file\n",
        "print(f\"JSON data has been saved to '{file_name}'.\")\n"
      ],
      "metadata": {
        "id": "MXAPQs8kHXz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **the whole thing with improvements, but this actually sucks lol**"
      ],
      "metadata": {
        "id": "MfPg2G3EdgJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "from aiohttp import ClientSession\n",
        "from lxml import html\n",
        "from urllib.parse import urljoin\n",
        "import json\n",
        "from cachetools import TTLCache, cached\n",
        "\n",
        "# Initialize cache with TTL (time-to-live) of 1 hour\n",
        "cache = TTLCache(maxsize=1000, ttl=3600)\n",
        "\n",
        "# Optimized function to parse languages\n",
        "async def parse_languages(session, url):\n",
        "    async with session.get(url) as response:\n",
        "        content = await response.text()\n",
        "        tree = html.fromstring(content)\n",
        "\n",
        "        # Optimized XPath to find languages and their links\n",
        "        languages = tree.xpath('//p[contains(@class, \"lang--\")]/a/text()')\n",
        "        languages = [lang.strip() for lang in languages if lang.strip()]\n",
        "\n",
        "        language_links = tree.xpath('//p[contains(@class, \"lang--\")]/a/@href')\n",
        "        base_url = \"https://www.ethnologue.com\"\n",
        "        full_language_links = [urljoin(base_url, link) for link in language_links]\n",
        "\n",
        "        return languages, full_language_links\n",
        "\n",
        "# Optimized function to find links to other pages\n",
        "async def find_links_to_other_pages(session, url):\n",
        "    async with session.get(url) as response:\n",
        "        content = await response.text()\n",
        "        tree = html.fromstring(content)\n",
        "\n",
        "        links = tree.xpath('//button[@class=\"tab__link\"]/@onclick')\n",
        "        links = [link.split('\"')[1] for link in links if 'browse' in link]\n",
        "        full_links = [urljoin(url, f'/browse/names/{link}') for link in links]\n",
        "\n",
        "        return full_links\n",
        "\n",
        "# Async function to scrape language information\n",
        "@cached(cache)\n",
        "@cached(cache)\n",
        "async def scrape_language_info(session, language_url):\n",
        "    try:\n",
        "        async with session.get(language_url) as response:\n",
        "            content = await response.text()\n",
        "            tree = html.fromstring(content)\n",
        "\n",
        "            data = {}\n",
        "\n",
        "            # Extracting summary\n",
        "            summary_element = tree.xpath('//section[contains(@class, \"summary\")]//p')\n",
        "            if summary_element:\n",
        "                data['summary'] = summary_element[0].text_content().strip()\n",
        "\n",
        "            # Extracting population\n",
        "            population_element = tree.xpath('//li[contains(@class, \"population__sizes\")]//div[contains(@class, \"graph__langpop\")]')\n",
        "            if population_element:\n",
        "                data['population'] = population_element[0].text_content().strip()\n",
        "\n",
        "            # Extracting language status\n",
        "            vitality_element = tree.xpath('//li[contains(@class, \"population__vitality\")]//li[contains(@class, \"histogram__datum\") and @data-count]')\n",
        "            for status in vitality_element:\n",
        "                if int(status.get('data-count', 0)) > 0:\n",
        "                    label = status.xpath('.//label')\n",
        "                    if label:\n",
        "                        data['language_status'] = label[0].text_content().strip()\n",
        "                        break\n",
        "            else:\n",
        "                data['language_status'] = 'Unknown'\n",
        "\n",
        "            # Example: Extracting region\n",
        "            region_element = tree.xpath('//section[contains(@class, \"region\")]//p')\n",
        "            if region_element:\n",
        "                data['region'] = region_element[0].text_content().strip()\n",
        "\n",
        "            # Example: Extracting dialects\n",
        "            dialects_element = tree.xpath('//section[contains(@class, \"dialects\")]//li')\n",
        "            if dialects_element:\n",
        "                data['dialects'] = [dialect.text_content().strip() for dialect in dialects_element]\n",
        "\n",
        "            # Example: Extracting language family\n",
        "            family_element = tree.xpath('//section[contains(@class, \"family\")]//p')\n",
        "            if family_element:\n",
        "                data['family'] = family_element[0].text_content().strip()\n",
        "\n",
        "            # Example: Extracting ISO code\n",
        "            iso_element = tree.xpath('//section[contains(@class, \"iso\")]//p')\n",
        "            if iso_element:\n",
        "                data['iso_code'] = iso_element[0].text_content().strip()\n",
        "\n",
        "            return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {language_url}: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Main function to manage the crawling process\n",
        "async def main(start_url):\n",
        "    visited = set()\n",
        "    to_visit = [start_url]\n",
        "    all_languages = []\n",
        "    all_full_language_links = []\n",
        "\n",
        "    async with ClientSession() as session:\n",
        "        while to_visit:\n",
        "            current_url = to_visit.pop()\n",
        "            if current_url in visited:\n",
        "                continue\n",
        "\n",
        "            visited.add(current_url)\n",
        "            languages, full_language_links = await parse_languages(session, current_url)\n",
        "            all_languages.extend(languages)\n",
        "            all_full_language_links.extend(full_language_links)\n",
        "\n",
        "            links = await find_links_to_other_pages(session, current_url)\n",
        "            to_visit.extend(links)\n",
        "\n",
        "        # Batch processing of language information scraping\n",
        "        tasks = [scrape_language_info(session, url) for url in all_full_language_links]\n",
        "        language_info_list = await asyncio.gather(*tasks)\n",
        "\n",
        "        # Combine language names with their respective information\n",
        "        language_info_dict = {name: info for name, info in zip(all_languages, language_info_list)}\n",
        "\n",
        "        # Convert the dictionary to JSON\n",
        "        json_data = json.dumps(language_info_dict, indent=2)\n",
        "\n",
        "        # Save JSON data to file\n",
        "        with open('language_data.json', 'w') as json_file:\n",
        "            json_file.write(json_data)\n",
        "\n",
        "        return json_data\n",
        "\n",
        "# Helper function to run the asyncio event loop in a Jupyter notebook\n",
        "def run_asyncio_coroutine(coroutine):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    if loop.is_running():\n",
        "        return loop.create_task(coroutine)\n",
        "    else:\n",
        "        return loop.run_until_complete(coroutine)\n",
        "\n",
        "# Entry point for the script\n",
        "if __name__ == '__main__':\n",
        "    start_url = 'https://www.ethnologue.com/browse/names'\n",
        "    task = run_asyncio_coroutine(main(start_url))\n",
        "    if isinstance(task, asyncio.Task):\n",
        "        task.add_done_callback(lambda t: print(t.result()))\n",
        "    else:\n",
        "        print(task)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "wf41CA3mdfwm",
        "outputId": "aafa1463-5e1c-4d39-bd06-0bcf79d4f412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2e1229385194>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mstart_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.ethnologue.com/browse/names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    }
  ]
}